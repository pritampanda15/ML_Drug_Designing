{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d3d22c8",
   "metadata": {},
   "source": [
    "# Dataset Splitting Strategy Notebook\n",
    "\n",
    "This notebook demonstrate how different settings were generated for the systematic study of a method's generalizability. With a specified split ratio, running through the whole notebook can give us the random split, unseen ligand scaffold split and unseen protein split setting for a given dataset. For unseen protein-ligand split, we refer our reader to the DrugBAN work (https://github.com/peizhenbai/DrugBAN). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "925f527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset csv file\n",
    "full_dataset_path = 'human_dataset.csv'\n",
    "\n",
    "## some initial parameters\n",
    "split_ratio = [0.7,0.1,0.2] \n",
    "output_directory = 'human'\n",
    "seed = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9899585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dddfd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.read_csv(full_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ed485",
   "metadata": {},
   "source": [
    "## Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c356e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fold(df, fold_seed, frac):\n",
    "    \"\"\"create random split\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataset dataframe\n",
    "        fold_seed (int): the random seed\n",
    "        frac (list): a list of train/valid/test fractions\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary of splitted dataframes, where keys are train/valid/test and values correspond to each dataframe\n",
    "    \"\"\"\n",
    "    train_frac, val_frac, test_frac = frac\n",
    "    test = df.sample(frac=test_frac, replace=False, random_state=fold_seed)\n",
    "    train_val = df[~df.index.isin(test.index)]\n",
    "    val = train_val.sample(\n",
    "        frac=val_frac / (1 - test_frac), replace=False, random_state=1\n",
    "    )\n",
    "    train = train_val[~train_val.index.isin(val.index)]\n",
    "\n",
    "    return {\n",
    "        \"train\": train.reset_index(drop=True),\n",
    "        \"valid\": val.reset_index(drop=True),\n",
    "        \"test\": test.reset_index(drop=True),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e9b2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = create_fold(full, seed, split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18d46790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "random_dir = os.path.join(output_directory, 'random')\n",
    "if not os.path.exists(random_dir):\n",
    "    os.makedirs(random_dir)\n",
    "\n",
    "splits['train'].to_csv(os.path.join(random_dir,'train.csv'),index=False)\n",
    "splits['valid'].to_csv(os.path.join(random_dir,'valid.csv'),index=False)\n",
    "splits['test'].to_csv(os.path.join(random_dir,'test.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d132145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7000166750041688\n",
      "0.10005002501250625\n",
      "0.19993329998332499\n"
     ]
    }
   ],
   "source": [
    "## CHECK ratio - random split usually gives almost the same ratio as the original specification\n",
    "print(pd.read_csv(os.path.join(random_dir,'train.csv')).shape[0]/full.shape[0])\n",
    "print(pd.read_csv(os.path.join(random_dir,'valid.csv')).shape[0]/full.shape[0])\n",
    "print(pd.read_csv(os.path.join(random_dir,'test.csv')).shape[0]/full.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4877edd2",
   "metadata": {},
   "source": [
    "## Unseen Ligand Scaffolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b69e32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scaffold_split(df, seed, frac, entity):\n",
    "    \"\"\"create scaffold split. it first generates molecular scaffold for each molecule and then split based on scaffolds\n",
    "    reference: https://github.com/chemprop/chemprop/blob/master/chemprop/data/scaffold.py\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataset dataframe\n",
    "        fold_seed (int): the random seed\n",
    "        frac (list): a list of train/valid/test fractions\n",
    "        entity (str): the column name for where molecule stores\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary of splitted dataframes, where keys are train/valid/test and values correspond to each dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        from rdkit import Chem\n",
    "        from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "        from rdkit import RDLogger\n",
    "\n",
    "        RDLogger.DisableLog(\"rdApp.*\")\n",
    "    except:\n",
    "        raise ImportError(\n",
    "            \"Please install rdkit by 'conda install -c conda-forge rdkit'! \"\n",
    "        )\n",
    "    from tqdm import tqdm\n",
    "    from random import Random\n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "    random = Random(seed)\n",
    "\n",
    "    s = df[entity].values\n",
    "    scaffolds = defaultdict(set)\n",
    "    idx2mol = dict(zip(list(range(len(s))), s))\n",
    "\n",
    "    error_smiles = 0\n",
    "    for i, smiles in tqdm(enumerate(s), total=len(s)):\n",
    "        try:\n",
    "            scaffold = MurckoScaffold.MurckoScaffoldSmiles(\n",
    "                mol=Chem.MolFromSmiles(smiles), includeChirality=False\n",
    "            )\n",
    "            scaffolds[scaffold].add(i)\n",
    "        except:\n",
    "            print_sys(smiles + \" returns RDKit error and is thus omitted...\")\n",
    "            error_smiles += 1\n",
    "\n",
    "    train, val, test = [], [], []\n",
    "    train_size = int((len(df) - error_smiles) * frac[0])\n",
    "    val_size = int((len(df) - error_smiles) * frac[1])\n",
    "    test_size = (len(df) - error_smiles) - train_size - val_size\n",
    "    train_scaffold_count, val_scaffold_count, test_scaffold_count = 0, 0, 0\n",
    "\n",
    "    # index_sets = sorted(list(scaffolds.values()), key=lambda i: len(i), reverse=True)\n",
    "    index_sets = list(scaffolds.values())\n",
    "    big_index_sets = []\n",
    "    small_index_sets = []\n",
    "    for index_set in index_sets:\n",
    "        if len(index_set) > val_size / 2 or len(index_set) > test_size / 2:\n",
    "            big_index_sets.append(index_set)\n",
    "        else:\n",
    "            small_index_sets.append(index_set)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(big_index_sets)\n",
    "    random.shuffle(small_index_sets)\n",
    "    index_sets = big_index_sets + small_index_sets\n",
    "\n",
    "    if frac[2] == 0:\n",
    "        for index_set in index_sets:\n",
    "            if len(train) + len(index_set) <= train_size:\n",
    "                train += index_set\n",
    "                train_scaffold_count += 1\n",
    "            else:\n",
    "                val += index_set\n",
    "                val_scaffold_count += 1\n",
    "    else:\n",
    "        for index_set in index_sets:\n",
    "            if len(train) + len(index_set) <= train_size:\n",
    "                train += index_set\n",
    "                train_scaffold_count += 1\n",
    "            elif len(val) + len(index_set) <= val_size:\n",
    "                val += index_set\n",
    "                val_scaffold_count += 1\n",
    "            else:\n",
    "                test += index_set\n",
    "                test_scaffold_count += 1\n",
    "\n",
    "    return {\n",
    "        \"train\": df.iloc[train].reset_index(drop=True),\n",
    "        \"valid\": df.iloc[val].reset_index(drop=True),\n",
    "        \"test\": df.iloc[test].reset_index(drop=True),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ed10e74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5997/5997 [00:02<00:00, 2593.37it/s]\n"
     ]
    }
   ],
   "source": [
    "scaffold_splits = create_scaffold_split(full, seed, split_ratio,'Ligand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9779110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "scaffold_dir = os.path.join(output_directory, 'scaffold')\n",
    "if not os.path.exists(scaffold_dir):\n",
    "    os.makedirs(scaffold_dir)\n",
    "\n",
    "scaffold_splits['train'].to_csv(os.path.join(scaffold_dir,'train.csv'),index=False)\n",
    "scaffold_splits['valid'].to_csv(os.path.join(scaffold_dir,'valid.csv'),index=False)\n",
    "scaffold_splits['test'].to_csv(os.path.join(scaffold_dir,'test.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "717768d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6998499249624812\n",
      "0.09988327497081874\n",
      "0.20026680006670003\n"
     ]
    }
   ],
   "source": [
    "## CHECK ratio - scaffold split usually gives almost the same ratio as the original specification as well\n",
    "print(pd.read_csv(os.path.join(scaffold_dir,'train.csv')).shape[0]/full.shape[0])\n",
    "print(pd.read_csv(os.path.join(scaffold_dir,'valid.csv')).shape[0]/full.shape[0])\n",
    "print(pd.read_csv(os.path.join(scaffold_dir,'test.csv')).shape[0]/full.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49388b4",
   "metadata": {},
   "source": [
    "## Unseen protein split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd04009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fold_setting_cold(df, fold_seed, frac, entities):\n",
    "    \"\"\"create cold-split where given one or multiple columns, it first splits based on\n",
    "    entities in the columns and then maps all associated data points to the partition\n",
    "\n",
    "    Args:\n",
    "            df (pd.DataFrame): dataset dataframe\n",
    "            fold_seed (int): the random seed\n",
    "            frac (list): a list of train/valid/test fractions\n",
    "            entities (Union[str, List[str]]): either a single \"cold\" entity or a list of\n",
    "                    \"cold\" entities on which the split is done\n",
    "\n",
    "    Returns:\n",
    "            dict: a dictionary of splitted dataframes, where keys are train/valid/test and values correspond to each dataframe\n",
    "    \"\"\"\n",
    "    if isinstance(entities, str):\n",
    "        entities = [entities]\n",
    "\n",
    "    train_frac, val_frac, test_frac = frac\n",
    "\n",
    "    # For each entity, sample the instances belonging to the test datasets\n",
    "    test_entity_instances = [\n",
    "        df[e]\n",
    "        .drop_duplicates()\n",
    "        .sample(frac=test_frac, replace=False, random_state=fold_seed)\n",
    "        .values\n",
    "        for e in entities\n",
    "    ]\n",
    "\n",
    "    # Select samples where all entities are in the test set\n",
    "    test = df.copy()\n",
    "    for entity, instances in zip(entities, test_entity_instances):\n",
    "        test = test[test[entity].isin(instances)]\n",
    "\n",
    "    if len(test) == 0:\n",
    "        raise ValueError(\n",
    "            \"No test samples found. Try another seed, increasing the test frac or a \"\n",
    "            \"less stringent splitting strategy.\"\n",
    "        )\n",
    "\n",
    "    # Proceed with validation data\n",
    "    train_val = df.copy()\n",
    "    for i, e in enumerate(entities):\n",
    "        train_val = train_val[~train_val[e].isin(test_entity_instances[i])]\n",
    "\n",
    "    val_entity_instances = [\n",
    "        train_val[e]\n",
    "        .drop_duplicates()\n",
    "        .sample(frac=val_frac / (1 - test_frac), replace=False, random_state=fold_seed)\n",
    "        .values\n",
    "        for e in entities\n",
    "    ]\n",
    "    val = train_val.copy()\n",
    "    for entity, instances in zip(entities, val_entity_instances):\n",
    "        val = val[val[entity].isin(instances)]\n",
    "\n",
    "    if len(val) == 0:\n",
    "        raise ValueError(\n",
    "            \"No validation samples found. Try another seed, increasing the test frac \"\n",
    "            \"or a less stringent splitting strategy.\"\n",
    "        )\n",
    "\n",
    "    train = train_val.copy()\n",
    "    for i, e in enumerate(entities):\n",
    "        train = train[~train[e].isin(val_entity_instances[i])]\n",
    "\n",
    "    return {\n",
    "        \"train\": train.reset_index(drop=True),\n",
    "        \"valid\": val.reset_index(drop=True),\n",
    "        \"test\": test.reset_index(drop=True),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d2ac38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_splits = create_fold_setting_cold(full,seed,split_ratio,'Protein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6e4f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "protein_dir = os.path.join(output_directory, 'protein')\n",
    "if not os.path.exists(protein_dir):\n",
    "    os.makedirs(protein_dir)\n",
    "\n",
    "protein_splits['train'].to_csv(os.path.join(protein_dir,'train.csv'),index=False)\n",
    "protein_splits['valid'].to_csv(os.path.join(protein_dir,'valid.csv'),index=False)\n",
    "protein_splits['test'].to_csv(os.path.join(protein_dir,'test.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9868c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6706686676671669\n",
      "0.10005002501250625\n",
      "0.22928130732032684\n"
     ]
    }
   ],
   "source": [
    "## CHECK ratio - protein split usually gives slight difference in ratio...\n",
    "print(pd.read_csv(os.path.join(protein_dir,'train.csv')).shape[0]/full.shape[0])\n",
    "print(pd.read_csv(os.path.join(protein_dir,'valid.csv')).shape[0]/full.shape[0])\n",
    "print(pd.read_csv(os.path.join(protein_dir,'test.csv')).shape[0]/full.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
